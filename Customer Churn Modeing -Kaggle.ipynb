{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data processing\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, KBinsDiscretizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_classif, VarianceThreshold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Experimental setup\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_validate, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train, test\n",
    "train = pd.read_csv('./data/orange_churn_train.csv', low_memory=False)\n",
    "test = pd.read_csv('./data/orange_churn_test.csv', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numerical and catergorical variables\n",
    "id_var = [\"cust_id\"]  # ID\n",
    "num_vars = train.columns[1:191].tolist()  # First 190 vars\n",
    "cat_vars = train.columns[191:231].tolist()  # Last 40 vars\n",
    "# Target get variable\n",
    "target_var = [\"churn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop constant variable: ['Var8', 'Var15', 'Var20', 'Var31', 'Var32', 'Var39', 'Var42', 'Var48', 'Var52', 'Var55', 'Var79', 'Var141', 'Var167', 'Var169', 'Var175', 'Var185', 'Var209', 'Var230']\n"
     ]
    }
   ],
   "source": [
    "# Count number of unique values of each variable\n",
    "vars_nunique = train[num_vars + cat_vars].apply(pd.Series.nunique, dropna=False, axis=0)\n",
    "cont_vars = vars_nunique.index[vars_nunique < 2].tolist()\n",
    "print(\"Drop constant variable:\", cont_vars)\n",
    "# Correct variable list\n",
    "num_vars = [v for v in num_vars if v not in cont_vars]\n",
    "cat_vars = [v for v in cat_vars if v not in cont_vars]\n",
    "# Update train, test\n",
    "train = train[id_var+num_vars+cat_vars+target_var]\n",
    "test = test[id_var+num_vars+cat_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - # NA of num vars: 1345535\n",
      "Train - # NA of cat vars: 79067\n",
      "Test - # NA of num vars: 1345234\n",
      "Test - # NA of cat vars: 79296\n"
     ]
    }
   ],
   "source": [
    "# Check missing value\n",
    "print('Train - # NA of num vars:', train[num_vars].isna().sum().sum())\n",
    "print('Train - # NA of cat vars:', train[cat_vars].isna().sum().sum())\n",
    "print('Test - # NA of num vars:', test[num_vars].isna().sum().sum())\n",
    "print('Test - # NA of cat vars:', test[cat_vars].isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = 0.7* len(train)\n",
    "colsDrop = []\n",
    "for i in train.columns:\n",
    "    if train[i].isna().sum() >=benchmark:\n",
    "        colsDrop.append(i)\n",
    "train = train.drop(columns=colsDrop)\n",
    "test = test.drop(columns=colsDrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in colsDrop:\n",
    "    if i in num_vars:\n",
    "        num_vars.remove(i)\n",
    "    if i in cat_vars:\n",
    "        cat_vars.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List dummary variables to track missing values imputation\n",
    "na_vars = []\n",
    "\n",
    "# Numerical variables\n",
    "# Build the missing value imputor using the mean\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=True)\n",
    "\n",
    "\n",
    "imp.fit(train[num_vars])\n",
    "# Reconstruct the list of vars + indicators\n",
    "na_vars = na_vars + [num_vars[v] + \"_na\" for v in imp.indicator_.features_]\n",
    "impute_vars = num_vars + na_vars\n",
    "# Apply on train, test\n",
    "train[impute_vars] = pd.DataFrame(imp.transform(train[num_vars]), columns=impute_vars)\n",
    "test[impute_vars] = pd.DataFrame(imp.transform(test[num_vars]), columns=impute_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "# Impute missing value using a new category \"Missing\"\n",
    "# Note: If the categorical vars are imputed by most_frequent, the indicators should be added\n",
    "train[cat_vars] = train[cat_vars].fillna('Missing')\n",
    "test[cat_vars] = test[cat_vars].fillna('Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables as integer values\n",
    "# Note: All the NA values were imputed previously\n",
    "enc = OrdinalEncoder()\n",
    "enc.fit(pd.concat([train[cat_vars], test[cat_vars]], axis=0))\n",
    "# Apply on train, test\n",
    "train[cat_vars] = enc.transform(train[cat_vars])\n",
    "test[cat_vars] = enc.transform(test[cat_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert bool variable to int\n",
    "train[na_vars] = train[na_vars].astype(np.int8)\n",
    "test[na_vars] = test[na_vars].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "churn        1.000000\n",
      "Var126_na    0.104843\n",
      "Var113       0.054660\n",
      "Var207       0.047613\n",
      "Var220       0.037850\n",
      "               ...   \n",
      "Var65       -0.047361\n",
      "Var7        -0.059302\n",
      "Var73       -0.073407\n",
      "Var218      -0.076438\n",
      "Var189      -0.087654\n",
      "Name: churn, Length: 115, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "corr_matrix = train.corr()\n",
    "print(corr_matrix[\"churn\"].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 115)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2232c820f40>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['churn'].value_counts().sort_index(ascending=False).plot(kind='barh', title = \"Target dstribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"Basetable_basic_preprocessing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple train and test - no reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.iloc[:,train.columns != 'churn'], train['churn'], test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Decision tree classifier\n",
    "def apply_dtree(X_train, X_test, y_train, y_test):\n",
    "    dtree_model = DecisionTreeClassifier()\n",
    "    param_grid = {'max_depth': np.arange(3, 10)}\n",
    "    dtree_gs = GridSearchCV(dtree_model, param_grid, cv=5)\n",
    "    dtree_model = dtree_gs.fit(X_train, y_train)\n",
    "    dtree_predictions = dtree_model.predict(X_test)\n",
    "    dtree_score = dtree_model.score(X_test, y_test)\n",
    "    roc = roc_auc_score(y_test, dtree_model.predict_proba(X_test)[:,1])\n",
    "    return dtree_gs, dtree_score,roc\n",
    "\n",
    "# K Nearest Neighbors Classifier\n",
    "def apply_knn(X_train, X_test, y_train, y_test):\n",
    "    print(\"Type train\", type(X_train))\n",
    "    knn = KNeighborsClassifier()\n",
    "    params_knn = {'n_neighbors': np.arange(1, 25)}\n",
    "    knn_gs = GridSearchCV(knn, params_knn, cv=5)\n",
    "    knn = knn_gs.fit(X_train, y_train)\n",
    "    knn_predictions = knn.predict(X_test)\n",
    "    knn_score = knn.score(X_test, y_test)\n",
    "    roc = roc_auc_score(y_test, knn.predict_proba(X_test)[:,1])\n",
    "    return knn_gs, knn_score,roc\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "def apply_gbc(X_train, X_test, y_train, y_test):\n",
    "    gbc_model = GradientBoostingClassifier()\n",
    "    parameters = {\"max_depth\":[3,5,8], \"n_estimators\":[10, 100]}\n",
    "    gbc_gs = GridSearchCV(gbc_model, parameters, cv=5)\n",
    "    gbc_model = gbc_gs.fit(X_train, y_train)\n",
    "    gbc_predictions = gbc_model.predict(X_test)\n",
    "    gbc_score = gbc_model.score(X_test, y_test)\n",
    "    roc = roc_auc_score(y_test, gbc_model.predict_proba(X_test)[:,1])\n",
    "    return gbc_gs, gbc_score,roc\n",
    "\n",
    "# Random Forest Classifier\n",
    "def apply_rfc(X_train, X_test, y_train, y_test):\n",
    "    rfc_model = RandomForestClassifier(oob_score = True) \n",
    "    param_grid = {'n_estimators': [50, 100, 200, 400], 'max_features': ['auto', 'sqrt', 'log2']}\n",
    "    rfc_gs = GridSearchCV(estimator=rfc_model, param_grid=param_grid, cv= 5)\n",
    "    rfc_model = rfc_gs.fit(X_train, y_train)\n",
    "    rfc_predictions = rfc_model.predict(X_test)\n",
    "    roc = roc_auc_score(y_test, rfc_model.predict_proba(X_test)[:,1])\n",
    "    rfc_score = rfc_model.score(X_test, y_test)\n",
    "    return rfc_gs, rfc_score, roc\n",
    "\n",
    "# Linear regression\n",
    "def apply_logisticRegression(X_train, X_test, y_train, y_test):\n",
    "    lg =  LogisticRegression()\n",
    "    lg_model = lg.fit(X_train, y_train)\n",
    "    lg_predictions = lg_model.predict(X_test)\n",
    "    roc = roc_auc_score(y_test, lg_model.predict_proba(X_test)[:,1])\n",
    "    lg_score = lg_model.score(X_test, y_test)\n",
    "    return lg_model, lg_score, roc\n",
    "\n",
    "# Naive Bayes\n",
    "def apply_NB(X_train, X_test, y_train, y_test):\n",
    "    nb =  BernoulliNB()\n",
    "    nb_model = nb.fit(X_train, y_train)\n",
    "    nb_predictions = nb_model.predict(X_test)\n",
    "    roc = roc_auc_score(y_test, nb_model.predict_proba(X_test)[:,1])\n",
    "    nb_score = nb_model.score(X_test, y_test)\n",
    "    return nb_model, nb_score, roc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dtree with score: 0.923 ROC 0.6834908049696782 K FOLD [0.92  0.875 0.93  0.925 0.96  0.89  0.925 0.875 0.94  0.94 ]\n",
      "Type train <class 'pandas.core.frame.DataFrame'>\n",
      "Model knn with score: 0.923 ROC 0.5154651686341827 K FOLD [0.93  0.89  0.935 0.93  0.965 0.91  0.93  0.875 0.94  0.94 ]\n",
      "Model gbc with score: 0.9225 ROC 0.7629412840680446 K FOLD [0.93  0.89  0.93  0.93  0.965 0.915 0.93  0.875 0.945 0.94 ]\n",
      "Model rfc with score: 0.923 ROC 0.7127713835460315 K FOLD [0.93  0.89  0.935 0.93  0.965 0.915 0.93  0.875 0.945 0.94 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logisticRegression with score: 0.923 ROC 0.5215523912707012 K FOLD [0.93  0.89  0.935 0.93  0.965 0.915 0.93  0.875 0.94  0.94 ]\n",
      "Model NB with score: 0.8455 ROC 0.6263384502821123 K FOLD [0.81  0.73  0.845 0.76  0.82  0.83  0.87  0.78  0.84  0.745]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "methods = ['dtree', 'knn', 'gbc', 'rfc','logisticRegression', 'NB']\n",
    "resulted_models = {}\n",
    "resulted_scores = list()\n",
    "roc_vals = list()\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "for m in methods:\n",
    "    this_model, this_score,roc = eval('apply_'+m+'(X_train, X_test, y_train, y_test)')\n",
    "    resulted_models[m] = this_model\n",
    "    resulted_scores.append(this_score)\n",
    "    roc_vals.append(roc)\n",
    "    scores = cross_val_score(this_model, X_val,y_val, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    print('Model', m, 'with score:', this_score, \"ROC\", roc, \"K FOLD\",scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingClassifier(),\n",
       "             param_grid={'max_depth': [3, 5, 8], 'n_estimators': [10, 100]})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_chosen = resulted_models['gbc']\n",
    "model_chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.913925</td>\n",
       "      <td>0.086075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.914123</td>\n",
       "      <td>0.085877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.913925</td>\n",
       "      <td>0.086075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.886551</td>\n",
       "      <td>0.113449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.906186</td>\n",
       "      <td>0.093814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.886551</td>\n",
       "      <td>0.113449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.956510</td>\n",
       "      <td>0.043490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.886551</td>\n",
       "      <td>0.113449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.936001</td>\n",
       "      <td>0.063999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.929366</td>\n",
       "      <td>0.070634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1\n",
       "0     0.913925  0.086075\n",
       "1     0.914123  0.085877\n",
       "2     0.913925  0.086075\n",
       "3     0.886551  0.113449\n",
       "4     0.906186  0.093814\n",
       "...        ...       ...\n",
       "9995  0.886551  0.113449\n",
       "9996  0.956510  0.043490\n",
       "9997  0.886551  0.113449\n",
       "9998  0.936001  0.063999\n",
       "9999  0.929366  0.070634\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_test = model_chosen.predict_proba(test)\n",
    "prediction_test_df = pd.DataFrame(prediction_test)\n",
    "prediction_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['predicted'] = prediction_test_df[1]\n",
    "export_testpredd = test[['cust_id','predicted']]\n",
    "export_testpredd.to_csv(\"result_noReduction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de incercat varianta cu smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA REDUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PVE = 0.9999999088426877\n"
     ]
    }
   ],
   "source": [
    "# Build PCA and check the explained variance\n",
    "# Note: If the num vars were scaled (but the dummy were not) PVE is no longer correct\n",
    "predictors =train.loc[:, train.columns != 'churn'].columns\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "pca.fit(train[predictors])\n",
    "print(\"PVE =\", pca.explained_variance_ratio_.sum())\n",
    "# Transform train, test\n",
    "train_pca = pca.transform(train[predictors])\n",
    "test_pca = pca.transform(test[predictors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20)\n",
      "(10000, 20)\n"
     ]
    }
   ],
   "source": [
    "print(train_pca.shape)\n",
    "print(train_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_pca).to_csv(\"Basetable_basic_PCA_dim_reduction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pca, train['churn'], test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dtree with score: 0.9225 ROC 0.56356495617059 K FOLD [0.93  0.88  0.935 0.93  0.965 0.915 0.93  0.875 0.94  0.94 ]\n",
      "Type train <class 'numpy.ndarray'>\n",
      "Model knn with score: 0.923 ROC 0.5154651686341827 K FOLD [0.93  0.89  0.935 0.93  0.965 0.91  0.93  0.875 0.94  0.94 ]\n",
      "Model gbc with score: 0.923 ROC 0.5796316359696642 K FOLD [0.93  0.89  0.935 0.93  0.965 0.915 0.93  0.875 0.94  0.94 ]\n",
      "Model rfc with score: 0.9225 ROC 0.5491023061445597 K FOLD [0.93  0.89  0.935 0.93  0.965 0.915 0.93  0.875 0.945 0.94 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logisticRegression with score: 0.45 ROC 0.5959779656962756 K FOLD [0.455 0.565 0.555 0.565 0.575 0.485 0.54  0.475 0.59  0.565]\n",
      "Model NB with score: 0.923 ROC 0.5959762068916998 K FOLD [0.93  0.89  0.935 0.93  0.965 0.915 0.93  0.875 0.945 0.94 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "methods = ['dtree', 'knn', 'gbc', 'rfc','logisticRegression', 'NB']\n",
    "resulted_models = {}\n",
    "resulted_scores = list()\n",
    "roc_vals = list()\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "for m in methods:\n",
    "    this_model, this_score,roc = eval('apply_'+m+'(X_train, X_test, y_train, y_test)')\n",
    "    resulted_models[m] = this_model\n",
    "    resulted_scores.append(this_score)\n",
    "    roc_vals.append(roc)\n",
    "    scores = cross_val_score(this_model, X_val,y_val, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    print('Model', m, 'with score:', this_score, \"ROC\", roc, \"K FOLD\",scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA + smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(train_pca, train['churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X, y).to_csv(\"Basetable_basic_PCA_SMOTE.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dtree with score: 0.6540744738262277 ROC 0.7014725048454904 K FOLD [0.66576819 0.61725067 0.64420485 0.57142857 0.61994609 0.63881402\n",
      " 0.61351351 0.62432432 0.58918919 0.63243243]\n",
      "Type train <class 'numpy.ndarray'>\n",
      "Model knn with score: 0.865353480841878 ROC 0.8860677043515488 K FOLD [0.79514825 0.78975741 0.76819407 0.74393531 0.76280323 0.8032345\n",
      " 0.77027027 0.79189189 0.77567568 0.77567568]\n",
      "Model gbc with score: 0.8896384241770102 ROC 0.9551643243762751 K FOLD [0.84097035 0.78975741 0.82479784 0.79245283 0.80592992 0.78975741\n",
      " 0.8027027  0.83513514 0.78378378 0.80540541]\n",
      "Model rfc with score: 0.9131138694009714 ROC 0.9711661650073319 K FOLD [0.85983827 0.81401617 0.83557951 0.8032345  0.8032345  0.81401617\n",
      " 0.84594595 0.83783784 0.83513514 0.85405405]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logisticRegression with score: 0.5582838640043173 ROC 0.5768657140235991 K FOLD [0.56603774 0.57951482 0.56603774 0.54447439 0.56334232 0.56334232\n",
      " 0.56216216 0.57297297 0.5972973  0.54054054]\n",
      "Model NB with score: 0.5693470048569886 ROC 0.5851743284390605 K FOLD [0.54716981 0.56603774 0.56334232 0.54986523 0.54447439 0.52021563\n",
      " 0.57297297 0.60810811 0.58918919 0.54594595]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "methods = ['dtree', 'knn', 'gbc', 'rfc','logisticRegression', 'NB']\n",
    "resulted_models = {}\n",
    "resulted_scores = list()\n",
    "roc_vals = list()\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "for m in methods:\n",
    "    this_model, this_score,roc = eval('apply_'+m+'(X_train, X_test, y_train, y_test)')\n",
    "    resulted_models[m] = this_model\n",
    "    resulted_scores.append(this_score)\n",
    "    roc_vals.append(roc)\n",
    "    scores = cross_val_score(this_model, X_val,y_val, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    print('Model', m, 'with score:', this_score, \"ROC\", roc, \"K FOLD\",scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train.loc[:, train.columns != 'churn']\n",
    "y = train['churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=1))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LASSO REGRESSION\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.fillna(0))\n",
    "\n",
    "sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l2'))\n",
    "sel_.fit(scaler.transform(X_train.fillna(0)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 114\n",
      "selected features: 35\n",
      "features with coefficients shrank to zero: 0\n"
     ]
    }
   ],
   "source": [
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "      np.sum(sel_.estimator_.coef_ == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removed_feats = X_train.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\n",
    "removed_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filtered  =train[selected_feat]\n",
    "test_filtered  =test[selected_feat]\n",
    "# train_filtered_noid = train_filtered.drop(columns='cust_id')\n",
    "# test_filtered_noid = test_filtered.drop(columns='cust_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_filtered).to_csv(\"Basetable_basic_LassoSelection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_filtered, train['churn'], test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dtree with score: 0.9215 AUC 0.710266845830226 K FOLD [0.93  0.875 0.925 0.925 0.965 0.915 0.925 0.875 0.94  0.94 ]\n",
      "Type train <class 'pandas.core.frame.DataFrame'>\n",
      "Model knn with score: 0.923 AUC 0.5433844324689395 K FOLD [0.93  0.89  0.935 0.93  0.965 0.915 0.93  0.875 0.945 0.94 ]\n",
      "Model gbc with score: 0.9225 AUC 0.741307987786861 K FOLD [0.93  0.89  0.935 0.93  0.965 0.915 0.93  0.87  0.945 0.94 ]\n",
      "Model rfc with score: 0.924 AUC 0.6716171152790871 K FOLD [0.93  0.89  0.935 0.93  0.965 0.915 0.93  0.875 0.945 0.94 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logisticRegression with score: 0.923 AUC 0.49945125297237974 K FOLD [0.93  0.89  0.935 0.93  0.965 0.915 0.93  0.875 0.945 0.94 ]\n",
      "Model NB with score: 0.919 AUC 0.6127516849347835 K FOLD [0.93  0.89  0.93  0.93  0.96  0.915 0.92  0.875 0.94  0.94 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "methods = ['dtree', 'knn', 'gbc', 'rfc','logisticRegression', 'NB']\n",
    "resulted_models = {}\n",
    "resulted_scores = list()\n",
    "roc_vals = list()\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "for m in methods:\n",
    "    this_model, this_score,roc = eval('apply_'+m+'(X_train, X_test, y_train, y_test)')\n",
    "    resulted_models[m] = this_model\n",
    "    resulted_scores.append(this_score)\n",
    "    roc_vals.append(roc)\n",
    "    scores = cross_val_score(this_model, X_val,y_val, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    print('Model', m, 'with score:', this_score, \"AUC\", roc, \"K FOLD\",scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher Score variable selection + SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FisherScore(bt, target_var, predictors):\n",
    "    \"\"\"\n",
    "    This function calculate the Fisher score of a variable.\n",
    "\n",
    "    Ref:\n",
    "    ---\n",
    "    Verbeke, W., Dejaeger, K., Martens, D., Hur, J., & Baesens, B. (2012). New insights\n",
    "    into churn prediction in the telecommunication sector: A profit driven data mining\n",
    "    approach. European Journal of Operational Research, 218(1), 211-229.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the unique values of dependent variable\n",
    "    target_var_val = bt[target_var].unique()\n",
    "    # Calculate FisherScore for each predictor\n",
    "    predictor_FisherScore = []\n",
    "    for v in predictors:\n",
    "        fs = np.abs(np.mean(bt.loc[bt[target_var]==target_var_val[0], v]) - np.mean(bt.loc[bt[target_var]==target_var_val[1], v])) / \\\n",
    "             np.sqrt(np.var(bt.loc[bt[target_var]==target_var_val[0], v]) + np.var(bt.loc[bt[target_var]==target_var_val[1], v]))\n",
    "        predictor_FisherScore.append(fs)\n",
    "    return predictor_FisherScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create several lists to handle variables\n",
    "id_var = ['cust_id']\n",
    "target_var = ['churn']\n",
    "predictors = [v for v in train.columns if v not in id_var + target_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictor</th>\n",
       "      <th>fisherscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Var126_na</td>\n",
       "      <td>0.271197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Var189</td>\n",
       "      <td>0.230800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Var73</td>\n",
       "      <td>0.217568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Var218</td>\n",
       "      <td>0.196487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Var7</td>\n",
       "      <td>0.164765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    predictor  fisherscore\n",
       "99  Var126_na     0.271197\n",
       "41     Var189     0.230800\n",
       "14      Var73     0.217568\n",
       "63     Var218     0.196487\n",
       "1        Var7     0.164765"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Fisher Score for all variable\n",
    "fs = FisherScore(train, target_var[0], predictors)\n",
    "fs_df = pd.DataFrame({\"predictor\":predictors, \"fisherscore\":fs})\n",
    "fs_df = fs_df.sort_values('fisherscore', ascending=False)\n",
    "fs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected # vars : 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Var126_na', 'Var189', 'Var73', 'Var218', 'Var7', 'Var113',\n",
       "       'Var207', 'Var65', 'Var13', 'Var193', 'Var144', 'Var229', 'Var210',\n",
       "       'Var220', 'Var126', 'Var205', 'Var140', 'Var228', 'Var81',\n",
       "       'Var125', 'Var74', 'Var195', 'Var72', 'Var6', 'Var134', 'Var197',\n",
       "       'Var28', 'Var200', 'Var221', 'Var119', 'Var109', 'Var112',\n",
       "       'Var173', 'Var203', 'Var211', 'Var160', 'Var132', 'Var24',\n",
       "       'Var222', 'Var6_na', 'Var21_na', 'Var81_na', 'Var119_na',\n",
       "       'Var144_na', 'Var208', 'Var22', 'Var21', 'Var35_na', 'Var132_na',\n",
       "       'Var22_na'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the top variables based on Fisher Score\n",
    "# top_fs_vars = fs_df[fs_df['fisherscore'] >= 0.03]['predictor'].values\n",
    "top_fs_vars = fs_df['predictor'].values[:50]\n",
    "print(\"Selected # vars :\", len(top_fs_vars))\n",
    "top_fs_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fisher = train[top_fs_vars]\n",
    "test_fisher = test[top_fs_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(train_fisher, train['churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2232d0396d0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.value_counts().sort_index(ascending=False).plot(kind='barh', title = \"Target dstribution after SMOTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train_fisher, train['churn'], test_size=0.2, random_state=1)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dtree with score: 0.8961144090663788 AUC 0.9380340778860701 K FOLD [0.88409704 0.83018868 0.85175202 0.85983827 0.85175202 0.84097035\n",
      " 0.88378378 0.84054054 0.83243243 0.87567568]\n",
      "Type train <class 'pandas.core.frame.DataFrame'>\n",
      "Model knn with score: 0.832164058283864 AUC 0.8322611366188015 K FOLD [0.74663073 0.75202156 0.7574124  0.70889488 0.72506739 0.74393531\n",
      " 0.71891892 0.76216216 0.77027027 0.7972973 ]\n",
      "Model gbc with score: 0.9554776038855909 AUC 0.9791239819373514 K FOLD [0.93800539 0.94339623 0.94339623 0.94070081 0.95687332 0.9245283\n",
      " 0.96756757 0.93513514 0.94054054 0.94054054]\n",
      "Model rfc with score: 0.956287101996762 AUC 0.9869261315730844 K FOLD [0.9245283  0.94339623 0.94070081 0.93530997 0.9541779  0.9083558\n",
      " 0.96216216 0.92972973 0.94324324 0.94054054]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logisticRegression with score: 0.5709660010793308 AUC 0.6003995800332305 K FOLD [0.55795148 0.56873315 0.56873315 0.54716981 0.5902965  0.55525606\n",
      " 0.55945946 0.57297297 0.56486486 0.57837838]\n",
      "Model NB with score: 0.7037236913113869 AUC 0.7764017701511967 K FOLD [0.7115903  0.65768194 0.71967655 0.66037736 0.71428571 0.70619946\n",
      " 0.71081081 0.71891892 0.68378378 0.7       ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "methods = ['dtree', 'knn', 'gbc', 'rfc','logisticRegression', 'NB']\n",
    "resulted_models = {}\n",
    "resulted_scores = list()\n",
    "roc_vals = list()\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "for m in methods:\n",
    "    this_model, this_score,roc = eval('apply_'+m+'(X_train, X_test, y_train, y_test)')\n",
    "    resulted_models[m] = this_model\n",
    "    resulted_scores.append(this_score)\n",
    "    roc_vals.append(roc)\n",
    "    scores = cross_val_score(this_model, X_val,y_val, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    print('Model', m, 'with score:', this_score, \"AUC\", roc, \"K FOLD\",scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>19989</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>19992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>19994</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>19998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id  churn\n",
       "0           1      0\n",
       "1           2      0\n",
       "2           5      0\n",
       "3           6      0\n",
       "4          10      0\n",
       "...       ...    ...\n",
       "9995    19989      0\n",
       "9996    19992      0\n",
       "9997    19994      0\n",
       "9998    19998      0\n",
       "9999    20000      0\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = gbc_model.predict_proba(test)\n",
    "test['churn'] = prediction\n",
    "test['cust_id'] = test['cust_id']\n",
    "filtered = test[['cust_id','churn']]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.to_csv(\"result15.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 114 and input n_features is 115 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-231-7cfe118b305a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# test_filtered['cust_id'] = test['cust_id']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgbc_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprediction_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprediction_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    502\u001b[0m         \"\"\"\n\u001b[0;32m    503\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict_proba'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_estimator_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'estimator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1219\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0mcorresponds\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m         \"\"\"\n\u001b[1;32m-> 1221\u001b[1;33m         \u001b[0mraw_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1222\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raw_prediction_to_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1126\u001b[0m         \"\"\"\n\u001b[0;32m   1127\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1128\u001b[1;33m         \u001b[0mraw_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raw_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1129\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_raw_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_raw_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[1;34m\"\"\"Return the sum of the trees raw predictions (+ init estimator).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m         \u001b[0mraw_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raw_predict_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m         predict_stages(self.estimators_, X, self.learning_rate,\n\u001b[0;32m    620\u001b[0m                        raw_predictions)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_raw_predict_init\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;34m\"\"\"Check input and compute raw predictions of the init estimator.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m             raise ValueError(\"X.shape[1] should be {0:d}, not {1:d}.\".format(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m             raise ValueError(\"Number of features of the model must \"\n\u001b[0m\u001b[0;32m    397\u001b[0m                              \u001b[1;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m                              \u001b[1;34m\"input n_features is %s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 114 and input n_features is 115 "
     ]
    }
   ],
   "source": [
    "# test_filtered['cust_id'] = test['cust_id']\n",
    "prediction = gbc_model.predict_proba(test)\n",
    "prediction_df = pd.DataFrame(prediction)\n",
    "prediction_df\n",
    "\n",
    "test['prediction'] = prediction_df[1]\n",
    "filtered = test[['cust_id','prediction']]\n",
    "filtered.to_csv(\"result24.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
